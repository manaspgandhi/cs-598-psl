{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05be8cca",
   "metadata": {},
   "source": [
    "CS 598 Practical Statistical Learning Fall 2025  \n",
    "Coding Assignment 2  \n",
    "Group: Manas Gandhi (manaspg2), Neeya Devan, Rahul Kasibhatla  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c164526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from abess import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966c0f7",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=773)\n",
    "beta = np.array([\n",
    "\t\t1, 0.5, 0, -0.5, -1, 1, 0.5, 2, 0, 0,\n",
    "\t\t0.1, 0.2, 2, 0, 0, 0, -2, 1, 0, 0\n",
    "\t], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1a8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "#a - Generate a data set with p = 20 (independent) covariates, \n",
    "# n = 1, 000 observations and an associated quantitative response vector generated according to the model\n",
    "def generate_simulated_data(n: int = 1000, p: int = 20):\n",
    "    \"\"\"\n",
    "\tGenerate X (n x p), beta (p,), and Y (n,) according to the assignment specification.\n",
    "\n",
    "\tReturns:\n",
    "\t\tX: shape (n, p)\n",
    "\t\tbeta: shape (p,)\n",
    "\t\tY: shape (n,)\n",
    "\t\"\"\"\n",
    "\t#beta vector from assignment\n",
    "    beta = np.array([\n",
    "\t\t1, 0.5, 0, -0.5, -1, 1, 0.5, 2, 0, 0,\n",
    "\t\t0.1, 0.2, 2, 0, 0, 0, -2, 1, 0, 0\n",
    "\t], dtype=float)\n",
    "\t\n",
    "    X = rng.standard_normal(size=(n, p))\n",
    "    epsilon = rng.standard_normal(n)\n",
    "\n",
    "    Y = X.dot(beta) + epsilon\n",
    "\t\n",
    "    return X, Y\n",
    "X, Y = generate_simulated_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2c3a31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#b - Randomly split your data set into a training set containing 200 observations \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#    and a test set containing 800 observations.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_train_test\u001b[39m(X: np.ndarray, Y: np.ndarray) -> \u001b[43mTuple\u001b[49m[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n\u001b[32m      4\u001b[39m \tindices = np.arange(X.shape[\u001b[32m0\u001b[39m])\n\u001b[32m      5\u001b[39m \trng.shuffle(indices)\n",
      "\u001b[31mNameError\u001b[39m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "#b - Randomly split your data set into a training set containing 200 observations \n",
    "#    and a test set containing 800 observations.\n",
    "def split_train_test(X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "\tindices = np.arange(X.shape[0])\n",
    "\trng.shuffle(indices)\n",
    "\ttrain_indices = indices[:200]\n",
    "\ttest_indices = indices[200:]\n",
    "\n",
    "\tX_train = X[train_indices]\n",
    "\tY_train = Y[train_indices]\n",
    "\t\n",
    "\tX_test = X[test_indices]\n",
    "\tY_test = Y[test_indices]\n",
    "\tprint(\"X_train shape:\", X_train.shape)\n",
    "\tprint(\"Y_train shape:\", Y_train.shape)\n",
    "\tprint(\"X_test shape:\", X_test.shape)\n",
    "\tprint(\"Y_test shape:\", Y_test.shape)\n",
    "\t\n",
    "\treturn X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = split_train_test(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c - Perform best subset selection on the training set, and plot the training set\n",
    "# MSE associated with the best model of each size. \n",
    "def best_subset_selection(X_train: np.ndarray, Y_train: np.ndarray, p: int) -> list[float]:\n",
    "\tsubset_mse = []\n",
    "\tfor k in range(1, p+1):\n",
    "\t\tmodel = LinearRegression(support_size=k)\n",
    "\t\tmodel.fit(X_train, Y_train)\n",
    "\t\tyhat = model.predict(X_train)\n",
    "\t\tmse = mean_squared_error(Y_train, yhat)\n",
    "\t\tsubset_mse.append(mse)\n",
    "\n",
    "\treturn subset_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e334ade",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_subset_selection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m p=\u001b[32m20\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_mse = \u001b[43mbest_subset_selection\u001b[49m(X_train, Y_train, p)\n\u001b[32m      3\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      4\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, p + \u001b[32m1\u001b[39m), training_mse, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_subset_selection' is not defined"
     ]
    }
   ],
   "source": [
    "p=20\n",
    "training_mse = best_subset_selection(X_train, Y_train, p)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, p + 1), training_mse, marker='o')\n",
    "plt.xticks(range(1, p + 1))\n",
    "plt.xlabel(\"Subset Size (k)\")\n",
    "plt.ylabel(\"Training MSE\")\n",
    "plt.title(\"Best Subset Selection Training MSE\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f388b29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \t\tsubset_mse.append(mse)\n\u001b[32m     11\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m subset_mse\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m testing_mse = compute_mse(\u001b[43mX_train\u001b[49m, Y_train, X_test, Y_test, p)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#d. Plot the test set MSE associated with the best model of each size\n",
    "def compute_mse(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, p:int) -> list[float]:\n",
    "\tsubset_mse = []\n",
    "\tfor k in range(1, p+1):\n",
    "\t\tmodel = LinearRegression(support_size=k)\n",
    "\t\tmodel.fit(X_train, Y_train)\n",
    "\t\tyhat = model.predict(X_test)\n",
    "\t\tmse = mean_squared_error(Y_test, yhat)\n",
    "\t\tsubset_mse.append(mse)\n",
    "\n",
    "\treturn subset_mse\n",
    "testing_mse = compute_mse(X_train, Y_train, X_test, Y_test, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b39f749",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testing_mse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtesting_mse\u001b[49m)\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      3\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, p + \u001b[32m1\u001b[39m), testing_mse, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'testing_mse' is not defined"
     ]
    }
   ],
   "source": [
    "print(testing_mse)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, p + 1), testing_mse, marker='o')\n",
    "plt.xticks(range(1, p + 1))\n",
    "plt.xlabel(\"Subset Size (k)\")\n",
    "plt.ylabel(\"Training MSE\")\n",
    "plt.title(\"Best Subset Selection Testing MSE\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d14c22bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testing_mse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#e. For which model size does the test set MSE take on its minimum value? Comment on your results.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m min_mse = \u001b[38;5;28mmin\u001b[39m(\u001b[43mtesting_mse\u001b[49m)\n\u001b[32m      3\u001b[39m best_size = testing_mse.index(min_mse) + \u001b[32m1\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe best model size is \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(best_size))\n",
      "\u001b[31mNameError\u001b[39m: name 'testing_mse' is not defined"
     ]
    }
   ],
   "source": [
    "#e. For which model size does the test set MSE take on its minimum value? Comment on your results.\n",
    "min_mse = min(testing_mse)\n",
    "best_size = testing_mse.index(min_mse) + 1\n",
    "print(\"The best model size is \" + str(best_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ceefb",
   "metadata": {},
   "source": [
    "f - How does the model at which the test set MSE is minimized compare to the\n",
    "true model used to generate the data? Comment on the coefficient values.\n",
    "\n",
    "The test set MSE takes on its minimum value at a subset size of 11. This makes a lot of sense, given that our beta has 12 values - this means our true model size is 12, so makes sense that the ideal subset size of 11. For the nonzero predictors, the estimated coefficients are probably pretty close to their true values.\n",
    "\n",
    "Additionally, two of the values in beta are 0.2 and 0.1, which are very small, so it's likely one of these that got missed. This is expected, however, because signals that weak are tough to reliably detect with only 200 training observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c20e855a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m         errors.append(error)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m errors\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m errors = coefficient_error(\u001b[43mX_train\u001b[49m, Y_train, beta, p)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#gCreate a plot displaying error and comment on what you observe. How does this compare to the test MSE plot from (d)?\n",
    "def coefficient_error(X_train: np.ndarray, Y_train: np.ndarray, beta_true: np.ndarray, p: int) -> list[float]:\n",
    "    errors = []\n",
    "\n",
    "    for r in range(1, p + 1):\n",
    "        model = LinearRegression(support_size=r)\n",
    "        model.fit(X_train, Y_train)\n",
    "        beta_hat = model.coef_\n",
    "        error = np.sum((beta_true - beta_hat) ** 2)\n",
    "        errors.append(error)\n",
    "\n",
    "    return errors\n",
    "errors = coefficient_error(X_train, Y_train, beta, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16a8c02c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'errors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43merrors\u001b[49m)\n\u001b[32m      2\u001b[39m min_error = \u001b[38;5;28mmin\u001b[39m(errors)\n\u001b[32m      3\u001b[39m best_size = errors.index(min_error) + \u001b[32m1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'errors' is not defined"
     ]
    }
   ],
   "source": [
    "print(errors)\n",
    "min_error = min(errors)\n",
    "best_size = errors.index(min_error) + 1\n",
    "print(\"Lowest error is at subset size \" + str(best_size))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, p + 1), errors, marker=\"o\")\n",
    "plt.xticks(range(1, p + 1))\n",
    "plt.xlabel(\"Subset size (r)\")\n",
    "plt.ylabel(r\"$\\sum_{j=1}^p (\\beta_j - \\hat{\\beta}_j^{(r)})^2$\")\n",
    "plt.title(\"Coefficient Estimation Error vs Subset Size\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17200654",
   "metadata": {},
   "source": [
    "What we can see above that there is a sharp dropoff from subset size 1-7, and reaches a minimum once again at 11. After that, the error flattens out, because the rest of the predictors are irrelevant or 0. The test MSE follows very similar behavior, reaching a minmimum at 11. This makes sense, beacuse, based on our beta array, we have 12 true nonzero coefficients, so that's around where the model should perform the best, and our error should be the lowest. As earlier, we had two very small coefficients, so the model likely had lower prediction accuracy on those because of the small training size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7678e9",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a9193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size                                                                                 Selected variables  Predicition error (mse)\n",
      "    1                                                                            (ontarget_scoring_att,)             6.521076e+11\n",
      "    2                                                                     (sub_on, ontarget_scoring_att)             6.295688e+11\n",
      "    3                                                              (sub_on, goals, ontarget_scoring_att)             6.243642e+11\n",
      "    4                                             (height, cm, weight, kg, sub_on, ontarget_scoring_att)             6.186855e+11\n",
      "    5                                      (height, cm, weight, kg, sub_on, goals, ontarget_scoring_att)             6.131527e+11\n",
      "    6                          (height, cm, weight, kg, sub_on, goals, aerial_won, ontarget_scoring_att)             6.091227e+11\n",
      "    7           (height, cm, weight, kg, sub_on, goals, aerial_won, ontarget_scoring_att, total_offside)             6.099922e+11\n",
      "    8 (height, cm, weight, kg, sub_on, goals, yellow_card, won_tackle, aerial_won, ontarget_scoring_att)             6.164107e+11\n"
     ]
    }
   ],
   "source": [
    "# (a) Fit a best subset selection algorithm to the data set and report the best model of each\n",
    "# model size (up to 8 variables, excluding the intercept) and their prediction errors. Make\n",
    "# sure that you simplify your output to only present the essential information.\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_df = pd.read_csv(\"mls_test.csv\")\n",
    "train_df = pd.read_csv(\"mls_train.csv\")\n",
    "\n",
    "y_train = train_df[\"salary\"]\n",
    "X_train = train_df.drop(columns=[\"salary\"])\n",
    "\n",
    "y_test = test_df[\"salary\"]\n",
    "X_test = test_df.drop(columns=[\"salary\"])\n",
    "\n",
    "results = []\n",
    "\n",
    "for k in range(1, 9): \n",
    "    best_mse = np.inf\n",
    "    best_vars = None\n",
    "    \n",
    "    for combo in itertools.combinations(X_train.columns, k):\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train[list(combo)], y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test[list(combo)])\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_vars = combo\n",
    "\n",
    "    results.append({\n",
    "        \"Size\": k,\n",
    "        \"Selected variables\": best_vars,\n",
    "        \"Predicition error (mse)\": best_mse\n",
    "    })\n",
    "\n",
    "\n",
    "results_a = pd.DataFrame(results)\n",
    "print(results_a.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1677ea8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to AIC-- Best Model: ['height, cm', 'weight, kg', 'sub_on', 'goals', 'yellow_card', 'won_tackle', 'aerial_won', 'ontarget_scoring_att'] | Train MSE: 296811761763.0 | Test MSE: 616410698050.296\n",
      "According to BIC-- Best Model: ['sub_on', 'ontarget_scoring_att'] | Train MSE: 309434842063.795 | Test MSE: 629568772715.585\n",
      "According to Cp-- Best Model: ['height, cm', 'weight, kg', 'sub_on', 'goals', 'yellow_card', 'won_tackle', 'aerial_won', 'ontarget_scoring_att'] | Train MSE: 296811761763.0 | Test MSE: 616410698050.296\n",
      "According to Adj-R2-- Best Model: ['height, cm', 'weight, kg', 'sub_on', 'goals', 'yellow_card', 'won_tackle', 'aerial_won', 'ontarget_scoring_att'] | Train MSE: 296811761763.0 | Test MSE: 616410698050.296\n"
     ]
    }
   ],
   "source": [
    "# (b) Using the models reported in part (a), which is the best model acording to: (i) AIC, (ii)\n",
    "# BIC, (iii) Cp-Mallows, and (iv) R2a? For each criterion, report the MSE for both training and testing data.\n",
    "\n",
    "# AIC\n",
    "best_aic, best_model_aic = np.inf, None\n",
    "for row in results_a.itertuples(index=False):\n",
    "    vars_ = list(row[1])\n",
    "    k = len(vars_)\n",
    "    lm = LinearRegression().fit(X_train[vars_], y_train)\n",
    "    yhat_train = lm.predict(X_train[vars_])\n",
    "    rss_train = np.sum((y_train - yhat_train)**2)\n",
    "    curr_aic = len(y_train) * np.log(rss_train/len(y_train)) + 2*k\n",
    "    \n",
    "    if curr_aic < best_aic:\n",
    "        best_aic = curr_aic\n",
    "        best_model_aic = (vars_,\n",
    "                          mean_squared_error(y_train, yhat_train),\n",
    "                          mean_squared_error(y_test, lm.predict(X_test[vars_])))\n",
    "\n",
    "print(\"According to AIC-- Best Model:\", best_model_aic[0], \"| Train MSE:\", round(best_model_aic[1],3), \"| Test MSE:\", round(best_model_aic[2],3))\n",
    "\n",
    "\n",
    "# BIC\n",
    "best_bic, best_model_bic = np.inf, None\n",
    "for row in results_a.itertuples(index=False):\n",
    "    vars_ = list(row[1])\n",
    "    k = len(vars_)\n",
    "    lm = LinearRegression().fit(X_train[vars_], y_train)\n",
    "    yhat_train = lm.predict(X_train[vars_])\n",
    "    rss_train = np.sum((y_train - yhat_train)**2)\n",
    "    bic = len(y_train) * np.log(rss_train/len(y_train)) + k*np.log(len(y_train))\n",
    "    if bic < best_bic:\n",
    "        best_bic = bic\n",
    "        best_model_bic = (vars_,\n",
    "                          mean_squared_error(y_train, yhat_train),\n",
    "                          mean_squared_error(y_test, lm.predict(X_test[vars_])))\n",
    "\n",
    "print(\"According to BIC-- Best Model:\", best_model_bic[0], \"| Train MSE:\", round(best_model_bic[1],3), \"| Test MSE:\", round(best_model_bic[2],3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cp\n",
    "full_model = LinearRegression().fit(X_train, y_train)\n",
    "rss_full = np.sum((y_train - full_model.predict(X_train))**2)\n",
    "sigma2_hat = rss_full / (len(y_train) - X_train.shape[1] - 1)\n",
    "\n",
    "best_cp, best_model_cp = np.inf, None\n",
    "for row in results_a.itertuples(index=False):\n",
    "    vars_ = list(row[1])\n",
    "    k = len(vars_)\n",
    "    lm = LinearRegression().fit(X_train[vars_], y_train)\n",
    "    yhat_train = lm.predict(X_train[vars_])\n",
    "    rss_train = np.sum((y_train - yhat_train)**2)\n",
    "    cp = rss_train/sigma2_hat + 2*k - len(y_train)\n",
    "\n",
    "    if cp < best_cp:\n",
    "        best_cp = cp\n",
    "        best_model_cp = (vars_,\n",
    "                         mean_squared_error(y_train, yhat_train),\n",
    "                         mean_squared_error(y_test, lm.predict(X_test[vars_])))\n",
    "\n",
    "print(\"According to Cp-- Best Model:\", best_model_cp[0],\"| Train MSE:\", round(best_model_cp[1],3), \"| Test MSE:\", round(best_model_cp[2],3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# R2\n",
    "tss = np.sum((y_train - np.mean(y_train))**2)\n",
    "\n",
    "best_r2, best_model_r2 = -np.inf, None\n",
    "for row in results_a.itertuples(index=False):\n",
    "    vars_ = list(row[1])\n",
    "    k = len(vars_)\n",
    "    lm = LinearRegression().fit(X_train[vars_], y_train)\n",
    "    yhat_train = lm.predict(X_train[vars_])\n",
    "    rss_train = np.sum((y_train - yhat_train)**2)\n",
    "    adj_r2 = 1 - (rss_train/(len(y_train)-k-1)) / (tss/(len(y_train)-1))\n",
    "    if adj_r2 > best_r2:\n",
    "        best_r2 = adj_r2\n",
    "        best_model_r2 = (vars_,\n",
    "                         mean_squared_error(y_train, yhat_train),\n",
    "                         mean_squared_error(y_test, lm.predict(X_test[vars_])))\n",
    "\n",
    "print(\"According to Adj-R2-- Best Model:\", best_model_r2[0], \"| Train MSE:\", round(best_model_r2[1],3), \"| Test MSE:\", round(best_model_r2[2],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75568112",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_BaseRidgeCV.__init__() got an unexpected keyword argument 'store_cv_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[32m      7\u001b[39m lambdas = np.logspace(-\u001b[32m3\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m100\u001b[39m)  \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ridge_cv = \u001b[43mRidgeCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_cv_values\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m ridge_cv.fit(X_train, y_train)\n\u001b[32m     12\u001b[39m best_lambda = ridge_cv.alpha_\n",
      "\u001b[31mTypeError\u001b[39m: _BaseRidgeCV.__init__() got an unexpected keyword argument 'store_cv_values'"
     ]
    }
   ],
   "source": [
    "# (c) Fit a ridge regression model to predict a player’s salary. Use cross-validation to select\n",
    "# the best regularization parameter λ.\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lambdas = np.logspace(-3, 5, 100)  \n",
    "\n",
    "ridge_cv = RidgeCV(alphas=lambdas, store_cv_values=True)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "best_lambda = ridge_cv.alpha_\n",
    "\n",
    "yhat_train = ridge_cv.predict(X_train)\n",
    "yhat_test = ridge_cv.predict(X_test)\n",
    "train_mse = mean_squared_error(y_train, yhat_train)\n",
    "test_mse = mean_squared_error(y_test, yhat_test)\n",
    "\n",
    "print(\"Best λ:\", best_lambda)\n",
    "print(\"Ridge Regression -- Train MSE:\", train_mse, \"| Test MSE:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d179924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) Fit a lasso regression model on the same data set. Identify which features are shrunk to\n",
    "# zero.\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "lambdas = np.logspace(-3, 5, 100)  \n",
    "\n",
    "lasso_cv = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LassoCV(alphas=np.logspace(-3, 5, 100), cv=5, max_iter=10000, random_state=42)\n",
    ")\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "best_lambda = lasso_cv.named_steps['lassocv'].alpha_\n",
    "\n",
    "yhat_train = lasso_cv.predict(X_train)\n",
    "yhat_test = lasso_cv.predict(X_test)\n",
    "train_mse = mean_squared_error(y_train, yhat_train)\n",
    "test_mse = mean_squared_error(y_test, yhat_test)\n",
    "coef = lasso_cv.named_steps['lassocv'].coef_\n",
    "features = X_train.columns\n",
    "zero_features = [f for f, c in zip(features, coef) if c == 0]\n",
    "\n",
    "print(\"Best λ:\", best_lambda)\n",
    "print(\"Lasso Regression -- Train MSE:\", round(train_mse,3),\"| Test MSE:\", round(test_mse,3))\n",
    "print(\"Features shrunk to zero:\", zero_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9522455",
   "metadata": {},
   "source": [
    "(e) Compare the performance of the models in (b) vs. ridge vs. lasso using the MSE. Which model would you recommend for predicting MLS player salaries and why?\n",
    "\n",
    "Based on MSE, I would recommend the subset 6-varaible model from part b with an MSe OF 6.091227 × 10^11."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
